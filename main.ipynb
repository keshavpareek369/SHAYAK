{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0795a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "load_dotenv()\n",
    "JSON_PATH = \"D:/gov-scheme-assistant-updated/threetry/schemes.json\"\n",
    "DB_DIR = \"rag_db\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1 ‚Äî LOAD JSON\n",
    "# ============================================================\n",
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "docs = []\n",
    "for entry in data:\n",
    "    kb = entry[\"knowledge_base_entry\"]\n",
    "    text_parts = [f\"Scheme: {kb.get('scheme','')}\", f\"Summary: {kb.get('summary','')}\"]\n",
    "\n",
    "    for section in [\"key_information\", \"all_extracted_sections\"]:\n",
    "        section_data = kb.get(section, {})\n",
    "        if isinstance(section_data, dict):\n",
    "            for key, val in section_data.items():\n",
    "                if isinstance(val, list):\n",
    "                    text_parts.extend(val)\n",
    "                elif isinstance(val, str):\n",
    "                    text_parts.append(val)\n",
    "\n",
    "    full_text = \"\\n\".join(text_parts).strip()\n",
    "    if full_text:\n",
    "        docs.append(Document(page_content=full_text,\n",
    "                             metadata={\"scheme\": kb.get(\"scheme\", \"Unknown\")}))\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(docs)} documents.\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2 ‚Äî SPLIT TEXT & BUILD VECTOR STORE\n",
    "# ============================================================\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"üìö Created {len(chunks)} chunks.\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "vectordb = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=DB_DIR)\n",
    "vectordb.persist()\n",
    "print(\"‚úÖ Vector database built successfully!\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3 ‚Äî BUILD KNOWLEDGE GRAPH\n",
    "# ============================================================\n",
    "G = nx.DiGraph()\n",
    "\n",
    "for entry in data:\n",
    "    kb = entry[\"knowledge_base_entry\"]\n",
    "    scheme = kb.get(\"scheme\", \"Unknown\")\n",
    "    G.add_node(scheme, type=\"scheme\")\n",
    "\n",
    "    key_info = kb.get(\"key_information\", {})\n",
    "    for key, val in key_info.items():\n",
    "        if isinstance(val, list):\n",
    "            for v in val:\n",
    "                node_name = v.strip()\n",
    "                if node_name:\n",
    "                    G.add_node(node_name, type=key)\n",
    "                    G.add_edge(scheme, node_name, relation=key)\n",
    "        elif isinstance(val, str):\n",
    "            node_name = val.strip()\n",
    "            if node_name:\n",
    "                G.add_node(node_name, type=key)\n",
    "                G.add_edge(scheme, node_name, relation=key)\n",
    "\n",
    "print(f\"üï∏ Graph built with {len(G.nodes)} nodes and {len(G.edges)} edges.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff42ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 4 ‚Äî RETRIEVAL FUNCTIONS\n",
    "# ============================================================\n",
    "def retrieve_from_vector(query, k=10):\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": k})\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "    return \"\\n\\n\".join([r.page_content for r in results])\n",
    "\n",
    "def query_graph(G, query):\n",
    "    \"\"\"Return list of schemes connected to matching attribute nodes.\"\"\"\n",
    "    keywords = query.lower().split()\n",
    "    matching_nodes = [n for n in G.nodes if any(k in n.lower() for k in keywords)]\n",
    "    related_schemes = set()\n",
    "    for node in matching_nodes:\n",
    "        for pred in G.predecessors(node):\n",
    "            if G.nodes[pred].get(\"type\") == \"scheme\":\n",
    "                related_schemes.add(pred)\n",
    "    return list(related_schemes)\n",
    "\n",
    "def hybrid_retrieve(query, G, vectordb):\n",
    "    \"\"\"Graph ‚Üí scheme filter ‚Üí detailed vector context.\"\"\"\n",
    "    graph_schemes = query_graph(G, query)\n",
    "    graph_context = \"\\n\".join(graph_schemes)\n",
    "    if not graph_context:\n",
    "        print(\"‚ö† No graph match found ‚Äî falling back to pure vector retrieval.\")\n",
    "        return retrieve_from_vector(query)\n",
    "    return retrieve_from_vector(graph_context)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5 ‚Äî LLM CALLER (Gemini / Grok)\n",
    "# ============================================================\n",
    "# def generate_with_llm(query, context, llm_choice=\"gemini\"):\n",
    "def generate_with_llm(query, context, llm_choice=\"gemini\"):\n",
    "\n",
    "    prompt = f\"Use the context below to answer the query.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    if llm_choice.lower() == \"grok\":\n",
    "        import requests\n",
    "        response = requests.post(\n",
    "            \"https://api.x.ai/v1/grok/completions\",\n",
    "            headers={\"Authorization\": f\"Bearer {os.getenv('GROK_API_KEY')}\"},\n",
    "            json={\"prompt\": prompt, \"max_tokens\": 300}\n",
    "        )\n",
    "        return response.json().get(\"text\", \"\")\n",
    "\n",
    "    elif llm_choice.lower() == \"gemini\":\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "        model = genai.GenerativeModel(\"gemini-2.0-flash-001\")\n",
    "        result = model.generate_content(prompt)\n",
    "        return result.text\n",
    "\n",
    "    else:\n",
    "        return \"‚ùå Invalid LLM choice (use 'grok' or 'gemini').\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8752a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 6 ‚Äî MAIN EXECUTION\n",
    "# ============================================================\n",
    "if _name_ == \"_main_\":\n",
    "    user_query = \"I am a girl doing B.Tech, family income 2 lakh, need scholarship\"\n",
    "\n",
    "    print(\"\\nüîé Retrieving context using Graph + Vector RAG ...\")\n",
    "    context = hybrid_retrieve(user_query, G, vectordb)\n",
    "\n",
    "    print(\"\\nü§ñ Generating final answer from LLM ...\")\n",
    "    answer = generate_with_llm(user_query, context, llm_choice=\"gemini\")\n",
    "\n",
    "    print(\"\\n=== üß† FINAL ANSWER ===\")\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db392346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_eligibility(client, eligibility_data):\n",
    "    eligible_schemes = []\n",
    "\n",
    "    for scheme, criteria in eligibility_data.items():\n",
    "        eligible = True\n",
    "\n",
    "        # Age check\n",
    "        age_min = criteria.get(\"age_min\")\n",
    "        age_max = criteria.get(\"age_max\")\n",
    "        if age_min is not None and client.get(\"age\") is not None and client[\"age\"] < age_min:\n",
    "            eligible = False\n",
    "        if age_max is not None and client.get(\"age\") is not None and client[\"age\"] > age_max:\n",
    "            eligible = False\n",
    "\n",
    "        # Gender check\n",
    "        scheme_gender = criteria.get(\"gender\")\n",
    "        client_gender = client.get(\"gender\")\n",
    "        if scheme_gender and client_gender and client_gender.lower() != scheme_gender.lower():\n",
    "            eligible = False\n",
    "\n",
    "        # Income check\n",
    "        income_max = criteria.get(\"income_max\")\n",
    "        client_income = client.get(\"income\")\n",
    "        if income_max is not None and client_income is not None and client_income > income_max:\n",
    "            eligible = False\n",
    "\n",
    "        # Education check\n",
    "        scheme_edu = criteria.get(\"education\")\n",
    "        client_edu = client.get(\"education\")\n",
    "        if scheme_edu and client_edu and scheme_edu.lower() not in client_edu.lower():\n",
    "            eligible = False\n",
    "\n",
    "        # Other conditions (if any)\n",
    "        for cond in criteria.get(\"other_conditions\", []):\n",
    "            if cond not in client.get(\"other_conditions\", []):\n",
    "                eligible = False\n",
    "\n",
    "        if eligible:\n",
    "            eligible_schemes.append(scheme)\n",
    "\n",
    "    return eligible_schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b80bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load keys from .env file if available\n",
    "load_dotenv()\n",
    "\n",
    "# --------------------------\n",
    "# CONFIGURATION\n",
    "# --------------------------\n",
    "JSON_PATH = \"D:/gov-scheme-assistant-updated/threetry/schemes.json\"\n",
    "DB_DIR = \"rag_db\"\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # lightweight & fast\n",
    "\n",
    "# --------------------------\n",
    "# STEP 1: LOAD AND PARSE JSON\n",
    "# --------------------------\n",
    "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "docs = []\n",
    "eligibility_data = {}\n",
    "\n",
    "for entry in data:\n",
    "    kb = entry[\"knowledge_base_entry\"]\n",
    "    text_parts = []\n",
    "\n",
    "    # Main fields\n",
    "    text_parts.append(f\"Scheme: {kb.get('scheme', '')}\")\n",
    "    text_parts.append(f\"Summary: {kb.get('summary', '')}\")\n",
    "\n",
    "    # Flatten nested fields (key_information, all_extracted_sections, etc.)\n",
    "    for section in [\"key_information\", \"all_extracted_sections\"]:\n",
    "        section_data = kb.get(section, {})\n",
    "        if isinstance(section_data, dict):\n",
    "            for key, value in section_data.items():\n",
    "                if isinstance(value, list):\n",
    "                    text_parts.extend(value)\n",
    "                elif isinstance(value, str):\n",
    "                    text_parts.append(value)\n",
    "\n",
    "    # Combine all text\n",
    "    full_text = \"\\n\".join(text_parts).strip()\n",
    "\n",
    "    # Create Document for embedding\n",
    "    if full_text:\n",
    "        docs.append(Document(page_content=full_text, metadata={\"scheme\": kb.get(\"scheme\", \"Unknown\")}))\n",
    "\n",
    "    # Extract structured eligibility info (handle missing fields)\n",
    "    key_info = kb.get(\"key_information\", {})\n",
    "    eligibility_data[kb.get(\"scheme\", \"Unknown\")] = {\n",
    "        \"age_min\": key_info.get(\"age_min\"),\n",
    "        \"age_max\": key_info.get(\"age_max\"),\n",
    "        \"gender\": key_info.get(\"gender\"),\n",
    "        \"income_max\": key_info.get(\"income_max\"),\n",
    "        \"education\": key_info.get(\"education\"),\n",
    "        \"other_conditions\": key_info.get(\"other_conditions\", [])\n",
    "    }\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents and prepared eligibility data.\")\n",
    "\n",
    "# --------------------------\n",
    "# STEP 2: SPLIT TEXT\n",
    "# --------------------------\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"Created {len(chunks)} text chunks for embedding.\")\n",
    "\n",
    "# --------------------------\n",
    "# STEP 3: CREATE EMBEDDINGS AND VECTOR STORE\n",
    "# --------------------------\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "vectordb = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=DB_DIR)\n",
    "vectordb.persist()\n",
    "print(\"‚úÖ Vector database built successfully!\")\n",
    "\n",
    "# --------------------------\n",
    "# STEP 4: RETRIEVAL FUNCTION\n",
    "# --------------------------\n",
    "def retrieve_context(query, k=10):\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": k})\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\\n\".join([r.page_content for r in results])\n",
    "    return context\n",
    "\n",
    "# --------------------------\n",
    "# STEP 5: LLM BACKENDS (GROK / GEMINI)\n",
    "# --------------------------\n",
    "def generate_with_llm(query, context, llm_choice=\"grok\"):\n",
    "    prompt = f\"Use the context below to answer the query.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    if llm_choice.lower() == \"grok\":\n",
    "        import requests\n",
    "        response = requests.post(\n",
    "            \"https://api.x.ai/v1/grok/completions\",\n",
    "            headers={\"Authorization\": f\"Bearer {os.getenv('GROK_API_KEY')}\"},\n",
    "            json={\"prompt\": prompt, \"max_tokens\": 300}\n",
    "        )\n",
    "        return response.json().get(\"text\", \"\")\n",
    "\n",
    "    elif llm_choice.lower() == \"gemini\":\n",
    "        from google.generativeai import GenerativeModel\n",
    "        import google.generativeai as genai\n",
    "        genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "        model = genai.GenerativeModel(\"gemini-2.0-flash-001\")\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "\n",
    "    else:\n",
    "        return \"‚ùå Invalid LLM choice. Use 'grok' or 'gemini'.\"\n",
    "\n",
    "# --------------------------\n",
    "# STEP 6: ELIGIBILITY CHECK\n",
    "# --------------------------\n",
    "def check_eligibility(client, eligibility_data):\n",
    "    eligible_schemes = []\n",
    "\n",
    "    for scheme, criteria in eligibility_data.items():\n",
    "        eligible = True\n",
    "\n",
    "        # Age check\n",
    "        age_min = criteria.get(\"age_min\")\n",
    "        age_max = criteria.get(\"age_max\")\n",
    "        if age_min is not None and client.get(\"age\") is not None and client[\"age\"] < age_min:\n",
    "            eligible = False\n",
    "        if age_max is not None and client.get(\"age\") is not None and client[\"age\"] > age_max:\n",
    "            eligible = False\n",
    "\n",
    "        # Gender check\n",
    "        scheme_gender = criteria.get(\"gender\")\n",
    "        client_gender = client.get(\"gender\")\n",
    "        if scheme_gender and client_gender and client_gender.lower() != scheme_gender.lower():\n",
    "            eligible = False\n",
    "\n",
    "        # Income check\n",
    "        income_max = criteria.get(\"income_max\")\n",
    "        client_income = client.get(\"income\")\n",
    "        if income_max is not None and client_income is not None and client_income > income_max:\n",
    "            eligible = False\n",
    "\n",
    "        # Education check\n",
    "        scheme_edu = criteria.get(\"education\")\n",
    "        client_edu = client.get(\"education\")\n",
    "        if scheme_edu and client_edu and scheme_edu.lower() not in client_edu.lower():\n",
    "            eligible = False\n",
    "\n",
    "        # Other conditions\n",
    "        for cond in criteria.get(\"other_conditions\", []):\n",
    "            if cond not in client.get(\"other_conditions\", []):\n",
    "                eligible = False\n",
    "\n",
    "        if eligible:\n",
    "            eligible_schemes.append(scheme)\n",
    "\n",
    "    return eligible_schemes\n",
    "\n",
    "# --------------------------\n",
    "# STEP 7: RUN QUERY FOR CLIENT\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    client_profile = {\n",
    "        \"age\": 20,\n",
    "        \"gender\": \"female\",\n",
    "        \"income\": 200000,\n",
    "        \"education\": \"B.Tech\",\n",
    "        \"other_conditions\": []\n",
    "    }\n",
    "\n",
    "    # Find eligible schemes\n",
    "    eligible_schemes = check_eligibility(client_profile, eligibility_data)\n",
    "    print(\"‚úÖ Eligible schemes for client:\", eligible_schemes)\n",
    "\n",
    "    # Retrieve context & get LLM answers for each eligible scheme\n",
    "    for scheme in eligible_schemes:\n",
    "        context = retrieve_context(scheme)\n",
    "        answer = generate_with_llm(scheme, context, llm_choice=\"gemini\")\n",
    "        print(f\"\\n=== {scheme} ===\")\n",
    "        print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
