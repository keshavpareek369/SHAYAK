{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244be6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader.py\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"https://www.myscheme.gov.in/api/v2/scheme/search\"\n",
    "\n",
    "def fetch_schemes(query: str, limit: int = 10):\n",
    "    params = {\"q\": query, \"limit\": limit}\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"schemes\", [])\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62968e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.py\n",
    "from langchain.schema import Document\n",
    "\n",
    "def get_scheme_docs(query: str):\n",
    "    schemes = fetch_schemes(query)\n",
    "    docs = []\n",
    "    for s in schemes:\n",
    "        title = s.get(\"title\", \"No Title\")\n",
    "        desc = s.get(\"shortDesc\", \"\")\n",
    "        url = f\"https://www.myscheme.gov.in/schemes/{s.get('schemeId')}\"\n",
    "        docs.append(\n",
    "            Document(\n",
    "                page_content=f\"{title}\\n{desc}\\nMore info: {url}\",\n",
    "                metadata={\"source\": url}\n",
    "            )\n",
    "        )\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb97d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "# from retriever import get_scheme_docs   # keep if you have retriever.py\n",
    "\n",
    "def build_chatbot(query: str):\n",
    "    docs = get_scheme_docs(query)\n",
    "\n",
    "    # ✅ HuggingFace embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # ✅ Chroma VectorDB (with persistence option)\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=\"./chroma_db\"   # folder to save db\n",
    "    )\n",
    "    retriever = vectordb.as_retriever()\n",
    "\n",
    "    # ✅ HuggingFace LLM (Flan-T5 as example)\n",
    "    hf_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=512)\n",
    "    llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "    # ✅ RetrievalQA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac29d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "# from chatbot import build_chatbot\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"education loan schemes\"\n",
    "    chatbot = build_chatbot(query)\n",
    "    result = chatbot.run(\"Tell me about available education loan schemes?\")\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb3626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_URL = \"https://www.myscheme.gov.in/search\"\n",
    "\n",
    "def scrape_schemes(query: str, limit: int = 5):\n",
    "    params = {\"q\": query}\n",
    "    response = requests.get(BASE_URL, params=params, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    schemes = []\n",
    "\n",
    "    # Each scheme card is inside <a href=\"/schemes/...\">\n",
    "    for card in soup.select(\"a[href^='/schemes']\")[:limit]:\n",
    "        title = card.select_one(\"h2, h3\")\n",
    "        desc = card.select_one(\"p\")\n",
    "        schemes.append({\n",
    "            \"title\": title.text.strip() if title else None,\n",
    "            \"description\": desc.text.strip() if desc else None,\n",
    "            \"url\": \"https://www.myscheme.gov.in\" + card[\"href\"]\n",
    "        })\n",
    "    return schemes\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    schemes = scrape_schemes(\"education\", limit=5)\n",
    "    for s in schemes:\n",
    "        print(f\"\\nTitle: {s['title']}\\nDescription: {s['description']}\\nURL: {s['url']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051c804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet -U langchain-scrapegraph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe48320",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb5d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapegraph_py.logger import sgai_logger\n",
    "import json\n",
    "\n",
    "from langchain_scrapegraph.tools import (\n",
    "    GetCreditsTool,\n",
    "    \n",
    "    MarkdownifyTool,\n",
    "    SmartCrawlerTool,\n",
    "    SmartScraperTool,\n",
    ")\n",
    "\n",
    "sgai_logger.set_logging(level=\"INFO\")\n",
    "\n",
    "smartscraper = SmartScraperTool()\n",
    "smartcrawler = SmartCrawlerTool()\n",
    "markdownify = MarkdownifyTool()\n",
    "credits = GetCreditsTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16463f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmartScraper\n",
    "result = smartscraper.invoke(\n",
    "    {\n",
    "        \"user_prompt\": \"Extract the company name and description\",\n",
    "        \"website_url\": \"https://www.myscheme.gov.in/search/category/Housing%20&%20Shelter\",\n",
    "    }\n",
    ")\n",
    "print(\"SmartScraper Result:\", result)\n",
    "\n",
    "# Markdownify\n",
    "markdown = markdownify.invoke({\"website_url\": \"https://www.myscheme.gov.in/search/category/Housing%20&%20Shelter\"})\n",
    "print(\"\\nMarkdownify Result (first 200 chars):\", markdown[:200])\n",
    "\n",
    "# SmartCrawler\n",
    "url = \"https://www.myscheme.gov.in/search/category/Housing%20&%20Shelter\"\n",
    "prompt = (\n",
    "    \"What does the company do? and I need text content from their privacy and terms\"\n",
    ")\n",
    "\n",
    "# Use the tool with crawling parameters\n",
    "result_crawler = smartcrawler.invoke(\n",
    "    {\n",
    "        \"url\": url,\n",
    "        \"prompt\": prompt,\n",
    "        \"cache_website\": True,\n",
    "        \"depth\": 2,\n",
    "        \"max_pages\": 2,\n",
    "        \"same_domain_only\": True,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nSmartCrawler Result:\")\n",
    "print(json.dumps(result_crawler, indent=2))\n",
    "\n",
    "# Check credits\n",
    "credits_info = credits.invoke({})\n",
    "print(\"\\nCredits Info:\", credits_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e9009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install crawl4ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b07bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl4ai-setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a864b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "\n",
    "async def main():\n",
    "    # Create an instance of AsyncWebCrawler\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        # Run the crawler on a URL\n",
    "        result = await crawler.arun(url=\"https://crawl4ai.com\")\n",
    "\n",
    "        # Print the extracted content\n",
    "        print(result.markdown)\n",
    "\n",
    "# Run the async main function\n",
    "asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
